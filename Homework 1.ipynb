{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe1bf2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#               0\n",
      "Name            0\n",
      "Type 1          0\n",
      "Type 2        386\n",
      "HP              0\n",
      "Attack          0\n",
      "Defense         0\n",
      "Sp. Atk         0\n",
      "Sp. Def         0\n",
      "Speed           0\n",
      "Generation      0\n",
      "Legendary       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset from the URL\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Get the count of missing values for each column\n",
    "missing_counts = df.isnull().sum()\n",
    "\n",
    "# Output the missing values count\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217fd97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 891 rows and 15 columns.\n",
      "         survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dataset with missing values\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Get the dimensions (rows and columns) of the dataset\n",
    "num_rows, num_columns = df.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f\"The dataset has {num_rows} rows and {num_columns} columns.\")\n",
    "\n",
    "# Get a summary of the dataset's columns\n",
    "column_summary = df.describe()\n",
    "\n",
    "# Print the summary\n",
    "print(column_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6cfd2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 891 rows and 15 columns.\n",
      "         survived      pclass         age       sibsp       parch        fare\n",
      "count  714.000000  714.000000  714.000000  714.000000  714.000000  714.000000\n",
      "mean     0.406162    2.236695   29.699118    0.512605    0.431373   34.694514\n",
      "std      0.491460    0.838250   14.526497    0.929783    0.853289   52.918930\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    1.000000   20.125000    0.000000    0.000000    8.050000\n",
      "50%      0.000000    2.000000   28.000000    0.000000    0.000000   15.741700\n",
      "75%      1.000000    3.000000   38.000000    1.000000    1.000000   33.375000\n",
      "max      1.000000    3.000000   80.000000    5.000000    6.000000  512.329200\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dataset without any missing values\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Get the dimensions (rows and columns) of the dataset\n",
    "num_rows, num_columns = df.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f\"The dataset has {num_rows} rows and {num_columns} columns.\")\n",
    "\n",
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        del df[col]\n",
    "# Get a summary of the dataset's columns\n",
    "column_summary = df.dropna().describe()\n",
    "\n",
    "# Print the summary\n",
    "print(column_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c03522",
   "metadata": {},
   "source": [
    "2.2: \n",
    "    The variables of a dataset are the categories of information it stores, usually corresponding with the headers of each colums. Examples could be color in a dataset of shirts, or brand in a dataset of cars. \n",
    "    The observations on the other hand are individual bits of information stored in a dataset, ususally corresponding with a row or even a single cell. In a dataset of shirts an observation could be a specific shirt or it's color, or for cars it could be a specific car or it's brand.\n",
    "    \n",
    "4:\n",
    "    The code Copilot returned had 'include=\"all\"' inside the describe function, meaning columns with any values were returned and there were the same amount as described by the size function. Without, only columns of integers were described and there were 6 as opposed to the 15 that the size function described, thus there were 9 columns of different types. \n",
    "    As for the descrepency between entry count and the number of rows, in the age column there are only 714 entries, meaning 177 people did not have their age recorded in the datasheet. All other numerical variables had every possible observation as a number, meaning the count was the same as the number of rows. \n",
    "    \n",
    "5:\n",
    "    Attributes are pieces of information that an 'object' has stored, usually a simple fixed value. Methods are things that an object can do. The method we used to summarise columns just prints out information, but they can also change information or do really anything, meanwhile an attribute cannot do much by itself. \n",
    "    \n",
    "6:\n",
    "    Count is the number of non null elements in the column, aka size minus the amount of missing data points. Mean is the average. Std is the standard deviation. Min is the lowest value. 25%, 50%, & 75% are all percentiles in terms of how large the number is. And Max in the highest value.\n",
    "    \n",
    "7:\n",
    "    \n",
    "    1. When there's only a small percent of missing elements in a column using dropna would keep more data, thus being preferred\n",
    "    2. When there are many missing datapoints in a column deleting the column entirely would keep more data, since dropna removes entire rows\n",
    "    3. If there is one row with many missing elements, and at least one other with relatively few missing elements, to keep the most usable data, deleting the entire problematic column, then combing through the rest with dropna would keep the most data.\n",
    "    4. Since df.describe() only runs on numeric columns, it was safe to fully clear all other columns to remove the missing values, so I iterated through and used del on all object type columns. Afterwords only the age column had missing values, and even though dropna results in slightly (within 5%) fewer total observations, I used it over deleting to column to have more complete data for the remaining people. Before and after can be seen by running the code above.\n",
    "    \n",
    "8:\n",
    "    \n",
    "    1. groupby gives statistics of a column sorted by the values of another column. For example, we could run the line df.groupby('age')['fare'].describe(), which would output describe's stats based on all the people at each age. For example it would give the average fare paid by 20 year olds.\n",
    "    2. Count counts the number of each value when using groupby, while it just counts the total number regularly. You can imagine that groupby makes another data frame with the columns being named after each value of the initial column.\n",
    "    3. For the most part, both means were enough to answer the questions. Google search, however, required more thought when in comes to what to search, while with the chat bot copy pasting the error was usually enough. Additionally, when it comes to more context dependant errors, or sometimes even with the most simple errors (like forgetting a bracket on a print statement), google would take multiple websites to fix the error, or sometimes never have anything helpful; meanwhile copilot answered everything perfectly. For a few cases though, I prefer google search's answer. Mosly, errors that are: 1. Relatively common for one reason or another 2. Required minimal other lines of code to understand & 3. Had multiple solutions. Case A was like this, since jupyter hub sometimes caused that specific error. The reason google's answer was better in that case was because the stack overflow page that was the top result had many different possible reasons, the same thing explained in many different wordings, and some niche & potentially more helpful solutions. Essentially, it allows for more browsing of options than just one chat bot promt. Overall though, AI was the best option, as sad as it makes me to say so.  \n",
    "    \n",
    "\n",
    "Summary Pt1: \n",
    "\n",
    "Absolutely! Here’s a summary of our conversation:\n",
    "\n",
    "    1. Python Script for Dataset Size: I provided a Python script using the pandas library to load the Titanic dataset from a URL and find its size (number of rows and columns).\n",
    "    2. Observations and Variables: I explained that observations are the rows (individual entries) in a dataset, and variables are the columns (attributes or characteristics of the observations).\n",
    "    3. Summary of Dataset Columns: I shared a Python script to get a statistical summary of the numerical columns and an overview of the dataset using the describe() and info() methods in pandas.\n",
    "    4. Attributes vs. Methods in Python: I clarified the difference between attributes (variables that store information about an object) and methods (functions that define behaviors or actions of an object) with examples.\n",
    "  \n",
    "Summary Pt2:\n",
    "    \n",
    "Sure! Here’s a summary of our conversation today:\n",
    "\n",
    "    1. Pandas describe Method: We discussed that std in the describe method stands for standard deviation, which measures the dispersion of a dataset.\n",
    "    2. Deleting a Column: You learned that using del df['col'] removes the specified column from the DataFrame.\n",
    "    3. Handling NaN Values: We talked about how df.dropna() removes entire rows that contain any null values by default.\n",
    "    4. GroupBy and Describe: We explored how df.groupby(\"col1\")[\"col2\"].describe() groups the DataFrame by col1 and computes descriptive statistics for col2 within each group.\n",
    "    5. Importing Pandas: You encountered an error name 'pd' is not defined, which was resolved by ensuring import pandas as pd is included at the top of your script.\n",
    "    6. HTTP Error 404: We discussed that this error indicates the server could not find the requested resource, often due to an incorrect URL.\n",
    "    7. Syntax and Typo Errors: We fixed a few syntax and typo errors in your code, such as missing parentheses in a print statement and correcting DF.describe() to df.describe().\n",
    "    8. NameError: We resolved a NameError by correctly referencing the column name as a string in the groupby method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7db9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
