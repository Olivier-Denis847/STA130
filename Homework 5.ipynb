{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad824de4",
   "metadata": {},
   "source": [
    "#### 1:\n",
    "If data can be statistically analyzed then it has already happened, or been tested. Data that cannot be statistically analyzed is both hard to run a trial of, and has no data on already.   \n",
    "A good null hypothesis is the common assumption about the mechanics of something that can be tested. In other words it says that there is nothing interesting that has happened with the data.   \n",
    "An alternative hypothesis in testing data takes a very small role, it's just an alternative option as opposed to the null hypothesis that is actually being tested. The alternative hypothesis is also not nearly as specific as the null hypothesis, and acts as a blanket statement for any other outcome.     \n",
    "\n",
    "#### 2:\n",
    "Statistics is always bound by how much data can be gathered, and how accurate it is. When trying to estimate something about the entire population of the earth, or even a more narrow category, it's practically impossible to track everything in that population. Instead we gather information from whoever we can, and the subsection of people we ask is called the sample.   \n",
    "Let's say you were trying to guess what percent of video games were in 3D, and you guessed that it was around 50%. If we look through 500 random videogames, that would be your sample, and if you take it's mean, that's your sample mean. What we're trying to estimate is whether or not half of all video games are 3D, not whether half of our sample is, meaning you need to somehow guess about the population of video games with only a small sample. Many different techniques are used to do this, but what's important to note is that we use the guess of 50% of video games being 3D to guess what a sample should be like. If our sample would be really rare with a 50% rate, then we can assume that the guess was wrong, and if our sample was pretty close then we can't really change anything about our guess. In other words, we use our sample to understand if the population is close to what we expected.    \n",
    "\n",
    "#### 3:\n",
    "A null hypothesis sample distribution is used to tell how odd any given sample distribution is. We assume that the null hypotesis is true because it gives an accurate guess as to how rare any given data is, and how certain we can be. If it were the other way around the bias of our sample would have a larger effect, and it rules out the option of the opposite correlation. It's just less scienfic to not assume the null hypotesis is true.\n",
    "\n",
    "#### 4:\n",
    "If we were in a world where our initial guess, the null hypothesis, was true, an unlikely (low p value) sample would require some absurd coincidences to happen for it to be what we got. For example if we expect winning the lottery to be a 99% chance every time we buy a ticket, and we still don't win after 1000 tickets, then it would be incredibly unlucky. In fact, the chances of losing that many times would be so absurdly unlikely that it's much more resonable to assume that the initial estimate was wrong, and not that we hit the (1/100^1000) chance to lose so many times. In other words, a low p value suggests that believing the null hypothesis is ridiculous.\n",
    "\n",
    "#### 6:\n",
    "Theoretically you could have a p value of exactly 0, like if you have a negative value that would otheriwse be graphed by a gamma distribution. But realistically p values if calculated correctly should all be above 0, meaning there's no absolute way to disprove something, just prove it to be unliekly enough to be unresonable.   \n",
    "\n",
    "#### 7:\n",
    "One tailed is just a test that cares about whether the result is more or less than the null hypothesis. If our observed stat was more than the null hypothesis, we would count how many values were more than it, and vise versa. All you need to do to change the code is remove the absolute values, although that doesn't always work.   \n",
    "\n",
    "Question 8 is below   \n",
    "I didn't use a chat bot to do this homework at all, so there's no summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f49c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p value is 0.0015\n",
      "Since 0.01 >= p > 0.001, we have strong evidence against the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "outliers = 0\n",
    "standard = 80/124\n",
    "n = 2000\n",
    "np.random.seed(45)\n",
    "\n",
    "for _ in range(n):\n",
    "    right = 0\n",
    "    for i in range(124):\n",
    "        right += np.random.choice([0,1])\n",
    "    if abs((right/124)-.5) >= abs((80/124)-.5):\n",
    "        outliers += 1\n",
    "\n",
    "print(f'p value is {outliers/n}')\n",
    "print('Since 0.01 >= p > 0.001, we have strong evidence against the null hypothesis.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac95c664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p value is 0.008.\n"
     ]
    }
   ],
   "source": [
    "#I can't be bothered to deal with the dependancies and variable names so i'm not copying the demo code\n",
    "import numpy as np\n",
    "\n",
    "null = 0.5\n",
    "observed = 0.8\n",
    "sign = 1 if observed > null else -1 #Kind of an odd line, but it makes sense\n",
    "\n",
    "n = 10\n",
    "outliers = 0\n",
    "for i in range(1000):\n",
    "    #Making simulated null hypothesis samples\n",
    "    tot = 0\n",
    "    for i in range(10):\n",
    "        tot += np.random.choice((0,1))\n",
    "    \n",
    "    #Count number of samples further from the null than what was observed \n",
    "    if sign * observed < sign * (tot/10):\n",
    "        outliers += 1\n",
    "\n",
    "print(f'The p value is {outliers/1000}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205a400",
   "metadata": {},
   "source": [
    "### Problem introduction    \n",
    "\n",
    "The story goes that famous statistician Ronald Fisher tested his colleague Dr. Muriel Bristol to see whether they could tell whether milk was poured before or after tea based on the taste. Fisher's colleague ended up getting 8 out of 8 trials correct, but when our STA130 class at U of T tried the same experiment, only 49 out of 80 people were able to guess the order of pouring. Now, it's our job to decide whether the students of STA130 are able to tell apart tea based on when the milk was poured. Formally, we want to test whether 49/80 correct guesses falls into a 95% confidence interval around a trial conducted under the assumption of the null hypothesis of exactly 50% correct guesses. In casual terms, our hypotheses are:  \n",
    "**Null hypothesis:** People can't tell apart the two methods of preparing tea   \n",
    "**Alternative hypothesis:** People can tell apart the two methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54811f3a",
   "metadata": {},
   "source": [
    "### Quantitative analysis  \n",
    "\n",
    "To analyse the hypothesis, we will write code to simulate a large number of trials with the odds predicted by the null hypothesis (50/50). Within each trial, we will take 80 hypothetical samples, and measure how many are correct guesses. Lastly, we have to track how many of these samples have a higher ratio of correct guesses than our original, and divide the total by the number of trials. If all goes well, we will have a p value that represents the likelyhood for our sample to have as or more extreme results from what we observed in a world where the null hypothesis was true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11673c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P value is: 0.0135\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample = 80\n",
    "original = 49\n",
    "\n",
    "#Using a one tailed test because we are only concerned with guessing right, not wrong\n",
    "above_count = 0\n",
    "trials = 2000\n",
    "np.random.seed(45)\n",
    "\n",
    "#Simulate 2000 trials\n",
    "for _ in range(trials):\n",
    "    correct_count = 0\n",
    "    \n",
    "    #Gather 80 samples\n",
    "    for i in range(sample):\n",
    "        #Simulate a 50% chance at either outcome\n",
    "        correct_count += np.random.choice((0,1))\n",
    "    \n",
    "    if correct_count > original:\n",
    "        above_count += 1\n",
    "\n",
    "p_val = above_count/trials\n",
    "print(f'P value is: {p_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f299a54",
   "metadata": {},
   "source": [
    "### Findings   \n",
    "\n",
    "The analysis ended up with a p value of 0.0135, which would mean there is good evidence against the null hypothesis. As for potential confounds, we don't know the details of whether the test was conducted accurately or not, and there is a sampling bias by only testing university students. That being said, with our current information we reject the null hypothesis, meaning that the general population is able to tell whether the milk or tea is poured first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
